# An Automated Infrastructure for Evaluating the Quality of LLM-Generated Unit Tests

The increasing adoption of Large Language Models (LLMs) in software development has introduced new possibilities for automating unit test generation. However, evaluating the quality of tests generated by these models remains a challenge, especially in scalable and reproducible ways. This paper presents an automated infrastructure for assessing the quality of unit tests generated by LLMs under different prompt strategies and model configurations. Our automation receives a production code snippet as input and generates test cases using multiple prompt types. The generated tests are then evaluated through a multi-criteria quality analysis that includes metrics such as test coverage, use of edge cases, diversity of assertions, detection of test smells, and function-level coverage. We compute a composite Test Quality Score (TQS) for each test based on these criteria. Automation supports open-source LLMs and is designed to scale with the addition of new models. We conducted an empirical study using the proposed automation with six LLMs: LLaMA 3.3, Gemma 3, WizardLM 2, and their respective variants specialized in code generation — CodeLLaMA, CodeGemma, and WizardCoder. 
The results indicate that most tests present TQS between 0 and 1, evidencing current limitations in generating high-quality tests. However, models specialized in code demonstrated superior performance compared to their generalist counterparts. We also observed that prompts with a few-shot strategy produced tests with higher scores, and the presence of docstrings in the input codes contributed positively to the quality of the generated tests. These findings highlight the importance of prompting strategies and rich semantic contexts to maximize the potential of LLMs in automated test generation. The proposed infrastructure provides a scalable and reproducible foundation for future research in this area.

### We introduce a fully automated pipeline that

1. **Generates unit tests** for any Python function/class using multiple open‑source LLMs.  
2. **Applies three prompt strategies** (*zero‑shot*, *few‑shot*, *chain‑of‑thought*).  
3. **Evaluates each test** on eight objective attributes (execution result, statement & function coverage, assert diversity, edge‑case handling, and six AST‑based test‑smells).  
4. **Computes a Test Quality Score (TQS)** and exports ready‑to‑analyze datasets.

The infrastructure is reproducible on local hardware, vendor‑neutral, and easily extensible to new models or metrics.

## Available Data

The data extracted from the empirical study is available in the ``dataset/`` directory
- ``input/`` are the prompts generated by the LLMs that the automation uses
- ``output_llms/`` are the responses from each LLM
- ``output/`` are the automation outputs with the quality assessment and TQS
- ``dataset. json`` is formed by joining the three outputs present in the above directories, it presents the final dataset of the research, containing the 522 cases evaluated
- ```dashboard.ipynb``` presents a notebook with all the data analysis presented in the result of the research, carried out through the dataset

## Quick Requirements

- Python ≥ 3.10
- ollama CLI
- coverage, flake8, pandas, seaborn, matplotlib
- 16 GB RAM (minimum) for 7-13 B models
- OS: Linux/macOS/Windows tested

## Quick-start (local execution)

1. Install the dependencies
```pip install -r requirements.txt``


2. Define the LLMs

 Install the LLMs with ollama ```ollama pull llama3.2``

   
 Open the file llms/generate_tests.py/ and define the llm you want, for example:
 ```
 "LLaMA3": lambda prompt: query_ollama(prompt, model='llama3. 2'),
 "CodeLLaMA": lambda prompt: query_ollama(prompt, model='codellama'),
 ```
 The 6 LLMs used in the search are already configured by default (LLaMa3, CodeLLaMa, Gemma, CodeGemma, WizardLM and WizardCoder)

3. Run ``app.py`` and submit the production code to the interface
4. After submitting the production code, a folder will be created with the results ```evaluation_results/```

(Processing time can be high, especially for machines outside the recommendations)

This README omits author names, affiliations and institutional URLs for review‑blindness.
